{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('pt1': conda)",
   "display_name": "Python 3.8.5 64-bit ('pt1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "606908fc0c26d26e63c369289d4b4eb7ec00ae86ec85f16e9e0fe916ae62f542"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 残差网络(ResNet)\n",
    "\n",
    "2015"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.残差块\n",
    "在残差块中,输入可通过跨层的数据线路更快地向前传播."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "ResNet``沿用了VGG``全3x3卷积层的设计.  \n",
    "残差块里首先有``两个``有相同输出通道数的``3x3卷积层``.每个卷积层后接一个``批量归一化层``和``ReLU激活函数``.  \n",
    "然后将输入跳过两个卷积运算后直接加在最后的ReLU激活函数前.  \n",
    "这样的设计要求,两个卷积层的``输出与输入形状一样``,从而可以相加.  \n",
    "若要改变通道数,就需要引入一个额外的1x1卷积层来将输入变换成需要的形状后再做相加运算.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差块\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return F.relu(Y + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "blk = Residual(3, 3)\n",
    "X = torch.rand((4, 3, 6, 6))\n",
    "blk(X).shape # torch.Size([4, 3, 6, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "blk = Residual(3, 6, use_1x1conv=True, stride=2) # 输出的宽高减半\n",
    "blk(X).shape"
   ]
  },
  {
   "source": [
    "## 2.ResNet模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "前两层跟之前介绍的GoogLeNet一样:在输出通道数为64, 步幅为2的7x7``卷积层``后接步幅为2的3x3的``最大池化层``.  \n",
    "不同之处在与ResNet每个``卷积层后增加的批量归一化层``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")"
   ]
  },
  {
   "source": [
    "GoogLeNet后面接了4个由Inception块组成的模块.  \n",
    "ResNet则使用``4个``由残差块组成的``模块``,  ``每个模块``使用``若干个``同样输出通道数的``残差块``.   \n",
    "\n",
    "\n",
    "第一个模块的通道数同输入通道数一致.由于之前已经使用了步幅为2的最大池化层,所以无序减小高和宽.之后的每个模块在第一个残差块里将上一个模块的通道数翻倍,并将高和宽减半."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差块组成的模块,第一个模块需要特别处理\n",
    "def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n",
    "    if first_block:\n",
    "        assert in_channels == out_channels # 第一个模块的通道数与输入通道数一致\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i==0 and not first_block:\n",
    "            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\n",
    "        else:\n",
    "            blk.append(Residual(out_channels, out_channels))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为ResNet加入所有残差块.这里每个模块使用两个残差块.\n",
    "net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n",
    "net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局平均池化\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入全局平均池化层后接上全连接层输出.\n",
    "net.add_module(\"global_avg_pool\", GlobalAvgPool2d())\n",
    "net.add_module(\"fc\", nn.Sequential(nn.Flatten(), nn.Linear(512, 10)))"
   ]
  },
  {
   "source": [
    "每个模块(resnet_block)里有4个卷积层(不算上1x1卷积层的话),加上最开始的卷积层和最后的全连接层,共计18层.这个模型通常被称为``ResNet-18``.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 output shape:\t torch.Size([1, 64, 112, 112])\n1 output shape:\t torch.Size([1, 64, 112, 112])\n2 output shape:\t torch.Size([1, 64, 112, 112])\n3 output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block1 output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block2 output shape:\t torch.Size([1, 128, 28, 28])\nresnet_block3 output shape:\t torch.Size([1, 256, 14, 14])\nresnet_block4 output shape:\t torch.Size([1, 512, 7, 7])\nglobal_avg_pool output shape:\t torch.Size([1, 512, 1, 1])\nfc output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 观察一下输入形状子啊ResNet不同模块之间的变化\n",
    "X = torch.rand((1, 1, 224, 224))\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "source": [
    "## 3.获取数据和训练模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print('train on', device)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # print(\"y.shape\", y.shape) # [128]\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item() # loss复制到cpu上\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_acc_sum, n_test = 0.0, 0 # 创建在内存(CPU)\n",
    "            for X_test, y_test in test_iter:\n",
    "                net.eval() # 评估模式\n",
    "                test_acc_sum += (net(X_test.to(device)).argmax(dim=1) == y_test.to(device)).sum().item()  # 对Tensor进行.item()取值后,得到的就是一个Python Scalar.\n",
    "                net.train() # 训练模式\n",
    "                n_test += y_test.shape[0]\n",
    "            test_acc = test_acc_sum / n_test\n",
    "\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "        % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train on cuda\n",
      "epoch 1, loss 0.4422, train acc 0.837, test acc 0.831, time 14.0 sec\n",
      "epoch 2, loss 0.3016, train acc 0.889, test acc 0.873, time 13.9 sec\n",
      "epoch 3, loss 0.2622, train acc 0.902, test acc 0.883, time 13.9 sec\n",
      "epoch 4, loss 0.2374, train acc 0.912, test acc 0.895, time 14.0 sec\n",
      "epoch 5, loss 0.2170, train acc 0.918, test acc 0.894, time 14.1 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}