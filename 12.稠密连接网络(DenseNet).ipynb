{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('pt1': conda)",
   "display_name": "Python 3.8.5 64-bit ('pt1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "606908fc0c26d26e63c369289d4b4eb7ec00ae86ec85f16e9e0fe916ae62f542"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 稠密连接网络(DenseNet)\n",
    "\n",
    "受到了ResNet中的跨层连接的启发."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**DenseNet与ResNet的区别?**  \n",
    "在ResNet中,跨层连接的两个输入输出形状必须相同,两个支路的输出直接相加.  \n",
    "在DenseNet中,两个输入输出是``在通道维度上连结``.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "DenseNet的主要构建模块是``稠密快(dense block)``和``过渡层(transition layer)``.  \n",
    "- 稠密快:定义了输入和输出是如何连结的.\n",
    "- 过渡层:用来控制通道数,使之不过大."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.稠密块\n",
    "DenseNet使用了ResNet改良版的\"批量归一化,激活,卷积\"结构."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels):\n",
    "    blk = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    )\n",
    "    return blk"
   ]
  },
  {
   "source": [
    "稠密块由多个conv_block组成,每块使用相同的输出通道数.在前向计算时,将每块的输入和输出在通道维上进行连结."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, in_channels, out_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        net = []\n",
    "        for i in range(num_convs):\n",
    "            in_c = in_channels + i * out_channels\n",
    "            net.append(conv_block(in_c, out_channels))\n",
    "        self.net = nn.ModuleList(net)\n",
    "        self.out_channels = in_channels + num_convs * out_channels # 计算输出通道数\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            X = torch.cat((X, Y), dim=1) # 在通道维上将输入和输出连结\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "blk = DenseBlock(num_convs=2, in_channels=3, out_channels=10)\n",
    "X = torch.rand(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "source": [
    "## 2.过渡层"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "由于每个稠密块都会带来通道数的增加,使用过多则会带来过于复杂的模型.  \n",
    "过渡层用来控制模型复杂度.  \n",
    "它通过1x1卷积层来减小通道数,并使用步幅为2的平均池化层减半高和宽,从而进一步降低模型复杂度."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(in_channels, out_channels):\n",
    "    blk = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=1), #  1x1卷积\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2) # 宽高减半\n",
    "    )\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# 对上一个例子中稠密块的输出使用通道数为10的过渡层.此时通道数减为10,高和宽减半.\n",
    "blk = transition_block(in_channels=23, out_channels=10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "source": [
    "## 3.DenseNet模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先使用同ResNet一样的单卷积层和最大池化层.\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2,  padding=1)\n",
    ")"
   ]
  },
  {
   "source": [
    "类似于ResNet的4个残差块,DenseNet接下来使用``4个稠密块``.  \n",
    "同ResNet一样,可以设置每个稠密块使用多少个卷积层,这里设为4.  \n",
    "稠密块里的卷积层通道数(``增长率``)设为32,所以``每个稠密块将增加128个通道``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels, groth_rate = 64, 32 # num_channels为当前的通道数\n",
    "num_convs_in_dense_block = [4, 4, 4, 4]\n",
    "for i, num_convs in enumerate(num_convs_in_dense_block):\n",
    "    DB = DenseBlock(num_convs, num_channels, groth_rate)\n",
    "    net.add_module(\"DenseBlock_%d\" % i, DB)\n",
    "    # 上一个稠密块的输出通道数\n",
    "    num_channels = DB.out_channels\n",
    "    # 在稠密块之间加入通道数减半的过渡层\n",
    "    if i != len(num_convs_in_dense_block) - 1:\n",
    "        net.add_module(\"transition_block_%d\" % i, transition_block(num_channels, num_channels // 2))\n",
    "        num_channels = num_channels // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局平均池化\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后接上全局平均池化层和全连接层来输出\n",
    "net.add_module(\"BN\", nn.BatchNorm2d(num_channels))\n",
    "net.add_module(\"relu\", nn.ReLU())\n",
    "net.add_module(\"global_avg_pool\", GlobalAvgPool2d())\n",
    "net.add_module(\"fc\", nn.Sequential(nn.Flatten(), nn.Linear(num_channels, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 output shape:\t torch.Size([1, 64, 48, 48])\n1 output shape:\t torch.Size([1, 64, 48, 48])\n2 output shape:\t torch.Size([1, 64, 48, 48])\n3 output shape:\t torch.Size([1, 64, 24, 24])\nDenseBlock_0 output shape:\t torch.Size([1, 192, 24, 24])\ntransition_block_0 output shape:\t torch.Size([1, 96, 12, 12])\nDenseBlock_1 output shape:\t torch.Size([1, 224, 12, 12])\ntransition_block_1 output shape:\t torch.Size([1, 112, 6, 6])\nDenseBlock_2 output shape:\t torch.Size([1, 240, 6, 6])\ntransition_block_2 output shape:\t torch.Size([1, 120, 3, 3])\nDenseBlock_3 output shape:\t torch.Size([1, 248, 3, 3])\nBN output shape:\t torch.Size([1, 248, 3, 3])\nrelu output shape:\t torch.Size([1, 248, 3, 3])\nglobal_avg_pool output shape:\t torch.Size([1, 248, 1, 1])\nfc output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 尝试打印每个子模块的输出维度确保网络无误\n",
    "X = torch.rand((1, 1, 96, 96))\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "source": [
    "## 4.获取数据训练模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print('train on', device)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # print(\"y.shape\", y.shape) # [128]\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item() # loss复制到cpu上\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_acc_sum, n_test = 0.0, 0 # 创建在内存(CPU)\n",
    "            for X_test, y_test in test_iter:\n",
    "                net.eval() # 评估模式\n",
    "                test_acc_sum += (net(X_test.to(device)).argmax(dim=1) == y_test.to(device)).sum().item()  # 对Tensor进行.item()取值后,得到的就是一个Python Scalar.\n",
    "                net.train() # 训练模式\n",
    "                n_test += y_test.shape[0]\n",
    "            test_acc = test_acc_sum / n_test\n",
    "\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "        % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = 96\n",
    "trans = []\n",
    "trans.append(torchvision.transforms.Resize(size=resize))\n",
    "trans.append(torchvision.transforms.ToTensor())\n",
    "transform = torchvision.transforms.Compose(trans) # 将两个变换串联起来\n",
    "\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=True, transform=transform)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train on cuda\n",
      "epoch 1, loss 0.4491, train acc 0.842, test acc 0.847, time 28.4 sec\n",
      "epoch 2, loss 0.2741, train acc 0.901, test acc 0.868, time 27.8 sec\n",
      "epoch 3, loss 0.2322, train acc 0.915, test acc 0.906, time 28.2 sec\n",
      "epoch 4, loss 0.2099, train acc 0.924, test acc 0.893, time 28.9 sec\n",
      "epoch 5, loss 0.1934, train acc 0.928, test acc 0.905, time 28.8 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  }
 ]
}