{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1603962436254",
   "display_name": "Python 3.8.5 64-bit ('pt1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5c7f384001939e2f7254925b843bcad48da5f89716d51c2cc623d0b1e05256db"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 《动手学深度学习》-Pytorch-笔记  \n",
    "https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter01_DL-intro/deep-learning-intro"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.创建tensor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]])\n"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(5,3)\n",
    "print(x)"
   ]
  },
  {
   "source": [
    "# 0到1之间的均匀分布随机数\n",
    "y = torch.rand(5, 3, dtype=torch.float)\n",
    "print(y)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.3978, 0.9225, 0.2420],\n        [0.1921, 0.6477, 0.5308],\n        [0.0065, 0.1388, 0.8290],\n        [0.1814, 0.4887, 0.7650],\n        [0.3092, 0.7973, 0.2034]])\n"
    }
   ]
  },
  {
   "source": [
    "### 指定输出"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1.0547, 1.6257, 1.7430],\n        [1.7060, 1.9304, 1.0602],\n        [1.6800, 1.9177, 1.7664],\n        [1.8921, 1.7341, 1.3179],\n        [1.8357, 1.2088, 1.8400]])\n"
    }
   ],
   "source": [
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y , out=result)\n",
    "print(result)"
   ]
  },
  {
   "source": [
    "### 对tensor进行数据索引，索引出来的的结果与原数据共享内存，也即修改一个，另一个会跟着修改。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([2.0547, 2.6257, 2.7430])\ntensor([[2.0547, 2.6257, 2.7430],\n        [1.7060, 1.9304, 1.0602],\n        [1.6800, 1.9177, 1.7664],\n        [1.8921, 1.7341, 1.3179],\n        [1.8357, 1.2088, 1.8400]])\n"
    }
   ],
   "source": [
    "xx = result[0, :]\n",
    "xx += 1\n",
    "print(xx)\n",
    "print(result)"
   ]
  },
  {
   "source": [
    "### 选择函数\n",
    "``index_select(input, dim, index)`` 在指定维度上进行选取，比如选取某些行，某些列。  \n",
    "``mask_select(input, mask)``  \n",
    "``nonzero(input)``                    非0元素的下标  \n",
    "``gather(input, dim, index)``         根据index，在dim维度上选取数据，输出的size与index一致。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 改变形状\n",
    "使用view()来改变Tensor的形状  \n",
    "注：view()返回的新Tensor与源Tensor虽然有不同的size，但是是共享内存的。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 返回一个新的副本（不共享内存）\n",
    "推荐先用``clone``创建一个副本，再使用``view``。  \n",
    "使用``clone``还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源``Tensor``。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 运行时的内存开销\n",
    "``y = x + y`` 这样的运算会开辟新内存。  \n",
    "不会开辟新内存的运算方式可写为如下：  \n",
    "``y\\[:\\] = y + x``  \n",
    "``torch.add(x, y, out=y)``  \n",
    "``y += x``  \n",
    "``y.add_(x)``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### tensor和numpy相互转换\n",
    "a = torch.ones(5)  \n",
    "b = a.numpy() \n",
    "   \n",
    "a = np.ones(5)  \n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "常用方法：  \n",
    "``c = torch.tensor(a)``  \n",
    "注： 这种方法会对数据进行拷贝，即返回的``Tensor``与原数据不共享内存。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Tensor on GPU\n",
    "使用方法``to()``将Tensor在CPU和GPU上相互移动"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "x = torch.ones(3,3)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device=device) # 直接创建一个在GPU上的tensor\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double)) # to()还可以同时改变数据类型"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], device='cuda:0')\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]], dtype=torch.float64)\n"
    }
   ]
  },
  {
   "source": [
    "### 自动求梯度\n",
    "Tensor是一个类，将其属性``.requires_grad``设置为True，则可对其进行track，进行反向传播。  \n",
    "``.detach()``将其从追踪记录中分离出来。  \n",
    "``with torch.no_grad()``将不想被追踪的操作代码块包裹起来，常用在评估模型的时候。  \n",
    "`` y.backward(w)``求的不是 y 对 x 的导数，而是 l = torch.sum(y*w) 对 x 的导数。w 可以视为 y 的各分量的权重，也可以视为遥远的损失函数 l 对 y 的偏导数。若 y 为标量，w 取默认值 1.0，才是按照我们通常理解的那样，求 y 对 x 的导数。  \n",
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把``梯度清零``。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}