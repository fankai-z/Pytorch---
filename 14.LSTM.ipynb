{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pt1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "cc7be5041237249e418598168f3d21261ff377f8931478ad5b74431d6b4bcfa7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## LSTM(Long short term memory)\n",
    "* 解决了梯度离散的问题.  \n",
    "* 解决了记忆长度的问题."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* Forget gate(遗忘门/记忆门)\n",
    "* Input gate(输入门)  \n",
    "* 输出门  \n",
    "\n",
    "Ct理解为`memory`，ht理解为`输出`。  \n",
    "有选择的过滤，有选择的输入，有选择的输出。  \n",
    "\n",
    "$i_{t}=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right)$  \n",
    "$\\tilde{C}_{t}=\\tanh \\left(W_{c} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{c}\\right)$  \n",
    "$C_{t}=f_{t} * C_{t-1}+i_{t} * \\tilde{C}_{t}$  \n",
    "\n",
    "$o_{t}=\\sigma\\left(W_{o}\\left[h_{t-1}, x_{t}\\right]+b_{o}\\right)$  \n",
    "$h_{t}=o_{t} * \\tanh \\left(C_{t}\\right)$  \n",
    "\n",
    "\n",
    "$\\begin{aligned}\\left(\\begin{array}{r}\\mathbf{i}^{(t)} \\\\ \\mathbf{f}^{(t)} \\\\ \\mathbf{o}^{(t)} \\\\ \\tilde{C}\\end{array}\\right) &=\\left(\\begin{array}{c}\\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\tanh \\end{array}\\right) \\mathbf{W}\\left(\\begin{array}{c}\\mathbf{x}^{(t)} \\\\ \\mathbf{h}^{(t-1)}\\end{array}\\right) \\\\ \\mathbf{c}^{(t)} &=\\mathbf{f}^{(t)} \\circ \\mathbf{c}^{(t-1)}+\\mathbf{i}^{(t)} \\circ \\tilde{C} \\\\ \\mathbf{h}^{(t)} &=\\mathbf{o}^{(t)} \\circ \\tanh \\left(\\mathbf{c}^{(t)}\\right) \\end{aligned}$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### LSTM类与LSTMCell"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "RNN:\n",
    "\n",
    "out, ht = rnn(x, h0)\n",
    "\n",
    "ht:`最后一个时间戳 所有层`的状态.  \n",
    "out:`所有时间戳 最后一层`的状态."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "LSTM:\n",
    "\n",
    "out, (ht, ct) = lstm(x, \\[ht_0, ct_0\\])  \n",
    "\n",
    "x: \\[seq, b, vec\\]  \n",
    "h/c: \\[num_layer, b, h\\]  \n",
    "out: \\[seq, b, h\\]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "LSTMCell:  \n",
    "\n",
    " ht, ct = lstmcell(xt, \\[ht_0, ct_0\\])\n",
    "\n",
    " xt:\\[b, vec\\]  \n",
    " ht/ct:\\[b, h\\]  \n",
    " \n",
    " Cell每次送\\[b, vec\\], 一共送seq次.  \n",
    " 返回结果没有了out项, 因为`out完全可以由ht叠加推导出来`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LSTM(100, 20, num_layers=4)\ntorch.Size([10, 3, 100])\ntorch.Size([10, 3, 20]) torch.Size([4, 3, 20]) torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(input_size=100, hidden_size=20, num_layers=4)\n",
    "print(lstm)\n",
    "x = torch.randn(10, 3, 100)\n",
    "print(x.shape)\n",
    "out, (h, c) = lstm(x) # 不传入h0,则默认为0.\n",
    "print(out.shape, h.shape, c.shape) # [10, 3, 20] [4, 3, 20] [4, 3, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "one layer lstm\ntorch.Size([3, 20]) torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "print('one layer lstm')\n",
    "cell = nn.LSTMCell(input_size=100, hidden_size=20)\n",
    "h = torch.zeros(3, 20)\n",
    "c = torch.zeros(3, 20)\n",
    "for xt in x:\n",
    "    h, c = cell(xt, [h, c])\n",
    "print(h.shape, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "two layer lstm\ntorch.Size([3, 20]) torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "print('two layer lstm')\n",
    "cell1 = nn.LSTMCell(input_size=100, hidden_size=30)\n",
    "cell2 = nn.LSTMCell(input_size=30, hidden_size=20)\n",
    "h1 = torch.zeros(3, 30)\n",
    "c1 = torch.zeros(3, 30)\n",
    "h2 = torch.zeros(3, 20)\n",
    "c2 = torch.zeros(3, 20)\n",
    "for xt in x:\n",
    "    h1, c1 = cell1(xt, [h1, c1])\n",
    "    h2, c2 = cell2(h1, [h2, c2])\n",
    "print(h2.shape, c2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}