{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('pt1': conda)",
   "display_name": "Python 3.8.5 64-bit ('pt1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "606908fc0c26d26e63c369289d4b4eb7ec00ae86ec85f16e9e0fe916ae62f542"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 含并行连结的网络(GoogLeNet)\n",
    "\n",
    "* 2014\n",
    "* 吸收了NiN网络中串联网络的思想,并做出很大改进.这里介绍这个系列模型的第一个版本."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### (1)Inception块\n",
    "\n",
    "* GoogLeNet中的基础卷积块,得名于同名电影<<盗梦空间>>(Inception).\n",
    "* Inception块里有4条并行的线路.  \n",
    "前3条使用窗口大小为1, 3, 5的卷积层来``抽取不同空间尺寸下的信息``,  \n",
    "中间2个线路会对输入先``做1x1卷积减少通道数``,以``降低模型复杂度``.  \n",
    "第4条线路使用``3x3的最大池化层``,后接``1x1卷积层来改变通道数``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Inception块中自定义的超参数是每个层的输出通道数,以此控制模型复杂度."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    # c1-c4为每条线路里层的输出通道数\n",
    "    def __init__(self, in_c, c1, c2, c3, c4):\n",
    "        super(Inception, self).__init__()\n",
    "        # 线路1,单1x1卷积层\n",
    "        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)\n",
    "        # 线路2,1x1卷积层后接3x3卷积层\n",
    "        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # 线路3,1x1卷积层后接5x5卷积层\n",
    "        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # 线路4,3x3最大池化层后接1x1卷积层\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1) # 在通道维度上连结输出.\n"
   ]
  },
  {
   "source": [
    "### (2)GoogLeNet模型\n",
    "\n",
    "主体卷积部分使用５个模块(block),模块之间使用步幅为2的最大池化层来减小输出高和宽.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个模块使用一个64通道的7x7卷积.\n",
    "b1 = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二个模块使用2个卷积层:首先是64通道的1x1卷积层,然后是将通道增大3倍的3x3卷积层.对应Inception块中的第二条线路."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(\n",
    "    nn.Conv2d(64, 64, kernel_size=1),\n",
    "    nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")"
   ]
  },
  {
   "source": [
    "第三个模块串联2个完整的Inception块.  \n",
    "第一个Inception块的输出通道数为64+128+32+32=25664+128+32+32=256.  \n",
    "第二个Inception块输出通道数增至128+192+96+64=480128+192+96+64=480.  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = nn.Sequential(\n",
    "    Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "    Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")"
   ]
  },
  {
   "source": [
    "第四模块串联了5个Inception块.  \n",
    "其输出通道数分别是  \n",
    "192+208+48+64=512  \n",
    "160+224+64+64=512    \n",
    "128+256+64+64=512  \n",
    "112+288+64+64=528112+288+64+64=528  \n",
    "256+320+128+128=832256+320+128+128=832  \n",
    "\n",
    "首先含3×33×3卷积层的``第二条线路输出最多通道``，其次是仅含1×11×1卷积层的第一条线路，之后是含5×55×5卷积层的第三条线路和含3×33×3最大池化层的第四条线路。其中第二、第三条线路都会先``按比例减小通道数``。这些``比例``在各个Inception块中都``略有不同``。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = nn.Sequential(\n",
    "    Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "    Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "    Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "    Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "    Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")"
   ]
  },
  {
   "source": [
    "第五模块有两个Inception块.  \n",
    "输出通道数为  \n",
    "256+320+128+128=832  \n",
    "384+384+128+128=1024  \n",
    "\n",
    "第五模块的后面``紧跟输出层``，该模块同NiN一样使用``全局平均池化层``来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个``输出个数为标签类别数``的全连接层。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局平均池化\n",
    "class GloablAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GloablAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b5 = nn.Sequential(\n",
    "    Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "    Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "    GloablAvgPool2d()\n",
    ")"
   ]
  },
  {
   "source": [
    "GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。这里我们将输入的高和宽从224降到96来简化计算。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "output shape: torch.Size([1, 64, 24, 24])\noutput shape: torch.Size([1, 192, 12, 12])\noutput shape: torch.Size([1, 480, 6, 6])\noutput shape: torch.Size([1, 832, 3, 3])\noutput shape: torch.Size([1, 1024, 1, 1])\noutput shape: torch.Size([1, 1024])\noutput shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5, nn.Flatten(), nn.Linear(1024, 10))\n",
    "X = torch.rand(1, 1, 96, 96)\n",
    "for blk in net.children():\n",
    "    X = blk(X)\n",
    "    print('output shape:', X.shape)"
   ]
  },
  {
   "source": [
    "### (3)获取数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用高和宽均为96像素的图像来训练GoogLeNet模型\n",
    "resize = 96\n",
    "trans = []\n",
    "trans.append(torchvision.transforms.Resize(size=resize))\n",
    "trans.append(torchvision.transforms.ToTensor())\n",
    "transform = torchvision.transforms.Compose(trans) # 将两个变换串联起来\n",
    "\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=True, transform=transform)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "source": [
    "### (4)训练模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print('train on', device)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # print(\"y.shape\", y.shape) # [128]\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item() # loss复制到cpu上\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_acc_sum, n_test = 0.0, 0 # 创建在内存(CPU)\n",
    "            for X_test, y_test in test_iter:\n",
    "                net.eval() # 评估模式\n",
    "                test_acc_sum += (net(X_test.to(device)).argmax(dim=1) == y_test.to(device)).sum().item()  # 对Tensor进行.item()取值后,得到的就是一个Python Scalar.\n",
    "                net.train() # 训练模式\n",
    "                n_test += y_test.shape[0]\n",
    "            test_acc = test_acc_sum / n_test\n",
    "\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "        % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "source": [
    "### (5)训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train on cuda\n",
      "epoch 1, loss 1.0218, train acc 0.609, test acc 0.823, time 44.5 sec\n",
      "epoch 2, loss 0.4298, train acc 0.844, test acc 0.858, time 44.4 sec\n",
      "epoch 3, loss 0.3484, train acc 0.872, test acc 0.876, time 44.5 sec\n",
      "epoch 4, loss 0.3056, train acc 0.887, test acc 0.882, time 44.6 sec\n",
      "epoch 5, loss 0.2737, train acc 0.900, test acc 0.893, time 44.6 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "source": [
    "* Inception块相当于一个有4条线路的子网络.通过不同窗口形状的卷积层和最大池化层来并行抽取信息,并使用1x1卷积层来减少通道数从而降低模型复杂度.\n",
    "* GoogLeNet将多个Inception块和其他层串联起来.其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的.\n",
    "* GoogLeNet系列一度是ImageNet上最高效的模型之一:在类似的测试精度下,它们的计算复杂度往往更低."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}