{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.0 64-bit ('base': conda)",
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5c7f384001939e2f7254925b843bcad48da5f89716d51c2cc623d0b1e05256db"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 1.深度学习基础"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 线性回归"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* 解析解(analytical solution)  \n",
    "误差最小化问题的解可以直接用公式表达出来\n",
    "* 数值解(numerical solution)  \n",
    "大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。\n",
    "* 小批量随机梯度下降（mini-batch stochastic gradient descent）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# len()返回的是第一个维度的值\n",
    "import torch\n",
    "x = torch.tensor([[-1.4239, -1.3788],\n",
    "        [ 0.0275,  1.3550],\n",
    "        [ 0.7616, -1.1384],\n",
    "        [ 0.2967, -0.1162],\n",
    "        [ 0.0822,  2.0826],\n",
    "        [-0.6343, -0.7222],\n",
    "        [ 0.4282,  0.0235],\n",
    "        [ 1.4056,  0.3506],\n",
    "        [-0.6496, -0.5202],\n",
    "        [-0.3969, -0.9951]])\n",
    "print(len(x))"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[ 0.7779,  1.7195],\n        [-1.1831, -1.3236]])\ntensor([[ 0.7779,  1.7195],\n        [-1.1831, -1.3236]])\n"
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(3, 2)\n",
    "print(torch.mm(a, b))\n",
    "print(torch.matmul(a, b))"
   ]
  },
  {
   "source": [
    "### 小批量随机梯度下降优化算法\n",
    "使用自动求梯度模块计算得到一个小批量的梯度和，再将其除以批量大小得到平均值"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 训练模型\n",
    "* 读取小批量数据样本(特征和标签)\n",
    "* 调用反向函数``backward``计算小批量梯度\n",
    "* 调用优化算法迭代模型参数\n",
    "(由于损失``l``不是一个标量,再次运行backward时需要先调用``.sum()``将损失求和变成一个标量)\n",
    "* 每次更新完参数需要将参数的梯度清零"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 训练部分代码\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要num_epochs个迭代周期\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X\n",
    "    # 和y分别是小批量样本的特征和标签\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y).sum()  # l是有关小批量X和y的损失\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度\n",
    "        sgd([w, b], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数\n",
    "\n",
    "        # 不要忘了梯度清零\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Pytorch的简洁实现"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 使用``data``包实现数据集的组合和读取"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 读取数据\n",
    "使用PyTorch的data包来实现特征和标签的组合``data.TensorDataset``,数据的读取``data.DataLoader``\n",
    "\n",
    "import torch.utils.data as Data\n",
    "\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "# 随机读取小批量\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### 定义模型\n",
    "* ``torch.nn``即nerual network.\n",
    "nn的核心数据结构是Module,常见做法为继承``nn.module``,撰写自己的额网络层."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* #### 使用``nn.module``实现一个线性回归模型\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(n_feature, 1)\n",
    "    # forward 定义前向传播\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "net = LinearNet(num_inputs)\n",
    "print(net) # 使用print可以打印出网络的结构\n"
   ]
  },
  {
   "source": [
    "* #### 使用``nn.Sequential``更加方便的搭建网络  \n",
    "Sequential是一个有序的容器,网络层按照传入容器中的顺序添加到计算图中."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* #### 通过``net.parameters()``查看模型的所有可学习参数  \n",
    "此函数返回一个生成器  \n",
    "for param in net.parameters():  \n",
    "print(param)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* #### 通过``init.normal_()``将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。偏差会初始化为零。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from torch.nn import init\n",
    "\n",
    "init.normal_(net[0].weight, mean=0, std=0.01)\n",
    "init.constant_(net[0].bias, val=0)  # 也可以直接修改bias的data: net[0].bias.data.fill_(0)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### 定义优化算法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* #### ``torch.optim``模块提供了很多常用的优化算法\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* 优化器实例，并指定学习率为0.03的小批量随机梯度下降（SGD）为优化算法。  \n",
    "import torch.optim as optim  \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.03)  \n",
    "print(optimizer)\n",
    "\n",
    "* 为不同子网络设置不同的学习率,如果对某个参数不指定学习率，就使用最外层的默认学习率  \n",
    "optimizer =optim.SGD([   \n",
    "      {'params': net.subnet1.parameters()}, # lr=0.03  \n",
    "{'params': net.subnet2.parameters(), 'lr': 0.01}  \n",
    "], lr=0.03)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* #### 注意,当需要修改学习率的时候\n",
    "\n",
    "主要有两种做法。一种是修改optimizer.param_groups中对应的学习率，另一种是更简单也是较为推荐的做法——新建优化器，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整学习率\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] *= 0.1 # 学习率为之前的0.1倍\n"
   ]
  },
  {
   "source": [
    "### 训练模型\n",
    "* 调用``optim``实例的``step``函数来迭代模型参数."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(output, y.view(-1, 1))\n",
    "        optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))"
   ]
  },
  {
   "source": [
    "### softmax回归\n",
    "对所有的分类类别输出做置信度估计,对每个类别的置信度取指数并统计每个类别的预测概率.softmax运算不改变预测类别输出.\n",
    "* 解决了两个问题  \n",
    "    * 将输出值变换成值为正且和为1的概率分布.   \n",
    "    * 输出值范围限定后,更易于衡量真实标签和确定范围的输出值之间的误差.  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 交叉熵损失函数\n",
    "* 为什么不选用平方损失?  \n",
    "平方损失过于严格.对于预测结果均正确的两个概率分布.正确标签为3.第一个预测概率输出为\\[0.2,0.2,0.6\\],第二个为\\[0,0.4,0.6\\].两者都有同样正确的分类预测结果,但是前者比后者的损失要小很多.过于严格,影响训练.\n",
    "\n",
    "* 如何改善上述问题?  \n",
    "使用更适合衡量两个概率分布差异的测量函数,其中,交叉熵（cross entropy）是一个常用的衡量方法.\n",
    "交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### torchvision包\n",
    "它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型.  \n",
    "主要由以下几部分构成:  \n",
    "\n",
    "``torchvision.datasets``:一些加载数据的函数及常用的数据集接口；  \n",
    "``torchvision.models``:包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；   \n",
    "``torchvision.transforms``:常用的图片变换，例如裁剪、旋转等；  \n",
    "``torchvision.utils``:含两个函数,一个是make_grid,它能将多张图片拼接在一个网格中;另一个是save_img,它能将tensor保存成图片. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* 使用torchvision.datasets来下载数据集.  \n",
    "* 参数transforms.ToTensor()  \n",
    "    * 将尺寸为(H,W,C)且数据位于\\[0, 255 \\]的PIL图片 或 数据类型为np.uint8的numpy数组----> 尺寸为(C,H,W),数据类型为torch.float32且位于\\[\\0.0, 1.0]的Tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchcvision.datasets提供常用的数据集加载\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "source": [
    "### torch.utils.data.DataLoader\n",
    "因为minst_train是``torch.utils.data.Dataset``的子类,所以可以将其传入``torch.utils.data.DataLoader``来创建数据样本的DataLoader实例.  \n",
    "PyTorch的``DataLoader``一个很方便的功能是允许使用多进程来加速数据的读取."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用DataLoader读取小批量数据\n",
    "\n",
    "batch_size = 256\n",
    "if sys.platform.startswith('win'):\n",
    "    num_workers = 0  # 0表示不用额外的进程来加速读取数据\n",
    "else:\n",
    "    num_workers = 4\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "source": [
    "#### 对于要忽略,不进行使用的变量,可以用下划线_进行接收.\n",
    "如: _, figs = plt.subplots(1, len(images), figsize=(12, 12))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 定义一个在一行里画多张图像和对应标签的函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "# 将数值标签转换成对应的文本文件\n",
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "# 在一行画多张图\n",
    "def show_fashion_mnist(images, labels):\n",
    "    d2l.use_svg_display()\n",
    "    # 这里的_表示我们忽略（不使用）的变量\n",
    "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n",
    "    for f, img, lbl in zip(figs, images, labels):\n",
    "        f.imshow(img.view((28, 28)).numpy())\n",
    "        f.set_title(lbl)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "# 看一下训练数据集中前10个样本的图像文件和文本标签.\n",
    "X, y = [], []\n",
    "for i in range(10):\n",
    "    X.append(mnist_train[i][0])\n",
    "    y.append(mnist_train[i][1])\n",
    "show_fashion_mnist(X, get_fashion_mnist_labels(y))\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### 多层感知机(multilayer perceptron, MLP)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}