{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('pt1': conda)",
   "display_name": "Python 3.8.5 64-bit ('pt1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "606908fc0c26d26e63c369289d4b4eb7ec00ae86ec85f16e9e0fe916ae62f542"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 网络中的网络(NiN)\n",
    "\n",
    "* 2014"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "LeNet,AlexNet,VGG共同之处是:先以卷积层构成的模块充分抽取空间特征,再以全连接层构成的模块来输出分类结果.  \n",
    "AlexNet和VGG对LeNet的改进主要在与对这两个模块进行``加宽``和``加深``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### (1)NiN块\n",
    "``使用1x1卷积层代替全连接层``.   \n",
    "NiN块由``一个卷积层+两个1x1卷积层``,其中第一个卷积层的超参数可以自行设定,第二个和第三个卷积层的超参数一般是固定的. \n",
    "\n",
    "\n",
    "1x1卷积层计算主要发生在通道维度上.  \n",
    "可以看成是将``通道维当成特征维``,将``宽和高维度``上的元素当成``数据样本``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 网络结构局部框架:  \n",
    "AlexNet,VGG: 卷积层 -> 卷积层 -> 全连接层 -> 全连接层  \n",
    "NiN: 卷积层 -> 1x1卷积层 -> 卷积层 -> 1x1卷积层"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return blk"
   ]
  },
  {
   "source": [
    "### (2)NiN模型\n",
    "* NiN使用卷积窗口形状分别为11,5和3的卷积层,相应的输出通道数也和AlexNet中的一致.每个NiN块后接一个步幅为2,窗口形状为3的最大池化层.\n",
    "* 去掉了AlexNet最后的3个全连接层,取而代之地,NiN使用了``输出通道数等于标签类别数``的NiN块,然后使用``全局平均池化``对每个通道中所有元素求平均并``直接用于分类``.  \n",
    "这里的全局平均池化即窗口形状等于输入空间维形状的平均池化层."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class GloablAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GloablAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "    \n",
    "net = nn.Sequential(\n",
    "    nin_block(1, 96, kernel_size=11, stride=4, padding=0),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Dropout(0.5),\n",
    "    # 标签数为10\n",
    "    nin_block(384, 10, kernel_size=3, stride=1, padding=1),\n",
    "    GloablAvgPool2d(),\n",
    "    nn.Flatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Sequential(\n    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n    (1): ReLU()\n    (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n    (5): ReLU()\n  )\n  (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (2): Sequential(\n    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (5): ReLU()\n  )\n  (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (4): Sequential(\n    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n    (5): ReLU()\n  )\n  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (6): Dropout(p=0.5, inplace=False)\n  (7): Sequential(\n    (0): Conv2d(384, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n    (5): ReLU()\n  )\n  (8): GloablAvgPool2d()\n  (9): Flatten()\n)\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 output shape: torch.Size([1, 96, 54, 54])\n1 output shape: torch.Size([1, 96, 26, 26])\n2 output shape: torch.Size([1, 256, 26, 26])\n3 output shape: torch.Size([1, 256, 12, 12])\n4 output shape: torch.Size([1, 384, 12, 12])\n5 output shape: torch.Size([1, 384, 5, 5])\n6 output shape: torch.Size([1, 384, 5, 5])\n7 output shape: torch.Size([1, 10, 5, 5])\n8 output shape: torch.Size([1, 10, 1, 1])\n9 output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 1, 224, 224)\n",
    "for name, blk in net.named_children():\n",
    "    X = blk(X)\n",
    "    print(name, 'output shape:', X.shape)\n"
   ]
  },
  {
   "source": [
    "### (3)获取数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = 224\n",
    "trans = []\n",
    "trans.append(torchvision.transforms.Resize(size=resize))\n",
    "trans.append(torchvision.transforms.ToTensor())\n",
    "transform = torchvision.transforms.Compose(trans) # 将两个变换串联起来\n",
    "\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=True, transform=transform)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "source": [
    "### (4)训练模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print('train on', device)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # print(\"y.shape\", y.shape) # [128]\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item() # loss复制到cpu上\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_acc_sum, n_test = 0.0, 0 # 创建在内存(CPU)\n",
    "            for X_test, y_test in test_iter:\n",
    "                net.eval() # 评估模式\n",
    "                test_acc_sum += (net(X_test.to(device)).argmax(dim=1) == y_test.to(device)).sum().item()  # 对Tensor进行.item()取值后,得到的就是一个Python Scalar.\n",
    "                net.train() # 训练模式\n",
    "                n_test += y_test.shape[0]\n",
    "            test_acc = test_acc_sum / n_test\n",
    "\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "        % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "source": [
    "### 训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train on cuda\n",
      "epoch 1, loss 1.2046, train acc 0.579, test acc 0.767, time 64.9 sec\n",
      "epoch 2, loss 0.5717, train acc 0.794, test acc 0.809, time 65.3 sec\n",
      "epoch 3, loss 0.4978, train acc 0.818, test acc 0.824, time 65.6 sec\n",
      "epoch 4, loss 0.4564, train acc 0.833, test acc 0.837, time 65.5 sec\n",
      "epoch 5, loss 0.4292, train acc 0.842, test acc 0.832, time 65.7 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.002, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "source": [
    "* NiN``重复使用``由``卷积层``和``代替全连接层的1x1卷积层``构成的NiN块,来``构建深层网络``.  \n",
    "* NiN去除了容易造成过拟合的全连接输出层,将其换成输出通道数等于标签类别数的NiN块和全局平均池化层.\n",
    "* NiN的以上设计思想影响了后面一系列卷积神经网络的设计."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}