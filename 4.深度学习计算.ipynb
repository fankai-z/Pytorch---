{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('pt1': conda)",
   "display_name": "Python 3.8.5 64-bit ('pt1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "606908fc0c26d26e63c369289d4b4eb7ec00ae86ec85f16e9e0fe916ae62f542"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1.模型构造"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.1 继承Module类来构造模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* 可以基于``Sequential类``构造模型\n",
    "* 也可以基于``Module类``构造模型\n",
    "\n",
    "``Module``类是``nn``模块里提供的一个模型构造类,是所有神经网络结构的``基类``.通过继承这个基类来定义我们想要的模型."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "这里定义的MLP类``重载``了Module类的``__init__``函数和``forward``函数.分别用于``初始化模型参数``和定义``前向计算``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        # 调用MLP父类Module的构造函数来进行必要的初始化.\n",
    "        # 使用**kwargs定义参数时，kwargs将会接收一个positional argument后所有关键词参数的字典。\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784, 256) # 隐藏层\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 10) # 输出层\n",
    "    \n",
    "    # 定义前向计算\n",
    "    # 目的是根据输入x计算模型输出.\n",
    "    def forward(self, x):\n",
    "        a = self.act(self.hidden(x))\n",
    "        return self.output(a)\n",
    "    "
   ]
  },
  {
   "source": [
    "MLP类中无需定义反向传播函数.系统通过自动求梯度而``自动生成``反向传播所需的``backward``函数."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "可通过``实例化MLP类``得到模型变量net.  \n",
    "net(x)会调用继承自Module类的__call__函数,这个函数将调用MLP类定义的forward函数完成前向计算.  \n",
    "\n",
    "也即,实例化之后,MLP类中的``前向计算会自动完成``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLP(\n  (hidden): Linear(in_features=784, out_features=256, bias=True)\n  (act): ReLU()\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0675, -0.1139, -0.0486, -0.2151, -0.0018, -0.0563, -0.0357,  0.1635,\n",
       "         -0.2710,  0.0487],\n",
       "        [-0.1617,  0.0408, -0.0064, -0.0279, -0.0036, -0.0329,  0.0648,  0.1680,\n",
       "         -0.1194,  0.2270]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X = torch.rand(2, 784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "source": [
    "Module类是一个可供自由组建的部件.它的子类既可以是一个层(如Linear类),又可以是一个模型(如上面的MLP类),或者是模型的一个部分."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.2 Module的子类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "继承自Module类的一些可以方便的构造模型的类还有:  \n",
    "* Sequential类  \n",
    "* ModuleList类  \n",
    "* ModuleDict类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### (1) Sequential类  \n",
    "以更简单的方式定义模型．它``接受``一个子模块的``有序字典``，或者，一系列子模块作为参数逐一添加``Ｍodule的实例``.在前向计算时就是将这些实例按添加顺序逐一计算."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### (2) ModuleList类  \n",
    "``接收``一个``子模块的列表``作为输入,然后也可以类似List那样进行append和extend操作.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\n",
    "# net.append(nn.Linear(256, 10)) # # 类似List的append操作\n",
    "# print(net[-1])  # 类似List的索引访问\n",
    "# print(net)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "source": [
    "#### Questions:  \n",
    "a) ``Sequential``和``MoudleList``的区别?  \n",
    "\n",
    "ModuleList仅仅储存各种模块的列表,模块之间没有联系,没有顺序,而且没有实现forward.  \n",
    "Sequential内的模块需要按照顺序排列,且要保证邻层的输入输出大小相匹配,内部forward功能已经实现.  \n",
    "\n",
    "b) ``ModuleList``的出现有什么作用?  \n",
    "\n",
    "让网络定义前向传播时更加灵活.  \n",
    "另外,加入到ModuleList中的模块会自动添加到网络中,看下面代码中的例子对比."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "net1:\ntorch.Size([10, 10])\ntorch.Size([10])\nnet2:\nModule_ModuleList(\n  (linears): ModuleList(\n    (0): Linear(in_features=10, out_features=10, bias=True)\n  )\n)\n=========\nModule_List()\n"
     ]
    }
   ],
   "source": [
    "class Module_ModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module_ModuleList, self).__init__()\n",
    "        # 加入到ModuleList中的模块自动添加到网络中.\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10)])\n",
    "\n",
    "class Module_List(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module_List, self).__init__()\n",
    "        # 这个线性层没有出现在网络结构中.\n",
    "        self.linears = [nn.Linear(10, 10)]\n",
    "\n",
    "net1 = Module_ModuleList()\n",
    "net2 = Module_List()\n",
    "\n",
    "print(\"net1:\")\n",
    "for p in net1.parameters():\n",
    "    print(p.size())\n",
    "\n",
    "print(\"net2:\")\n",
    "for p in net2.parameters():\n",
    "    print(p)\n",
    "\n",
    "print(net1)\n",
    "print(\"=========\")\n",
    "print(net2)\n"
   ]
  },
  {
   "source": [
    "#### (3) ModuleDict类  \n",
    "``接受``一个子模块的``字典``作为输入,然后可以类似字典那样进行添加和访问操作.  \n",
    "与ModuleList一样,仅仅存放了一个字典,forward函数需要自己实现,同时,ModuleDict里的模块也会自动添加到整个网络中."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ModuleDict(\n  (Linear): Linear(in_features=784, out_features=256, bias=True)\n  (act): ReLU()\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# 以字典作为输入\n",
    "net = nn.ModuleDict({\n",
    "    'Linear':nn.Linear(784, 256),\n",
    "    'act':nn.ReLU(),\n",
    "})\n",
    "# 添加\n",
    "net['output'] = nn.Linear(256, 10)\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "### 1.3 构造复杂的模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### (1)一个复杂一点的``FancyMLP``网络.  \n",
    "* 通过``get_constant``函数创建训练中不被迭代的参数,即``常数参数``.\n",
    "* 前向计算中,使用``Tensor``函数和Python的``控制流``,并多次``调用相同的层``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FancyMLP(\n  (linear): Linear(in_features=20, out_features=20, bias=True)\n)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(3.9904, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "class FancyMLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FancyMLP, self).__init__(**kwargs)\n",
    "\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False) # 不可训练参数(常数参数)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # 使用创建的常数参数,以及nn.functional中的relu函数和mm函数\n",
    "        x = nn.functional.relu(torch.mm(x, self.rand_weight.data) + 1)\n",
    "\n",
    "        # 复用全连接层.等价于两个全连接层共享参数\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # print(\"++++\")\n",
    "        # print(x.shape) #(2, 20)\n",
    "        # print(x.norm())\n",
    "        # print(x.norm().item())\n",
    "        while x.norm().item() > 1:\n",
    "            x /= 2\n",
    "        if x.norm().item() < 0.8:\n",
    "            x *= 10\n",
    "        return x.sum()\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net = FancyMLP()\n",
    "print(net)\n",
    "net(X)\n",
    "\n",
    "\n",
    "# 从结果看出,网络结构中只定义了一个linear层,但是前向计算中可以对linear层进行多次调用.\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "#### (2)定义多个继承自nn.Module的模块还可以作为Sequential的输入."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* 继承自Module类的模块可以作为``模型的一部分``,每个部分定义了``各自的前向计算过程``.每个部分可以作为Sequential的输入组合到一起,构成整个网络.\n",
    "\n",
    "* 印证前文所述:继承自Module类的子类是一个可供自由组建的部件.它可以是一个层,可以是一个模型,也可以是模型的一部分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): NestMLP(\n    (net): Sequential(\n      (0): Linear(in_features=40, out_features=30, bias=True)\n      (1): ReLU()\n    )\n  )\n  (1): Linear(in_features=30, out_features=20, bias=True)\n  (2): FancyMLP(\n    (linear): Linear(in_features=20, out_features=20, bias=True)\n  )\n)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-0.4560, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NestMLP, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential(nn.Linear(40, 30), nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "net = nn.Sequential(NestMLP(), nn.Linear(30, 20), FancyMLP())\n",
    "\n",
    "X = torch.rand(2, 40)\n",
    "print(net)\n",
    "net(X)\n"
   ]
  },
  {
   "source": [
    "## 2.模型参数的访问,初始化与共享"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " ### 2.1 访问模型参数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn中的init模块包含了多种模型初始化方法.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=4, out_features=3, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=3, out_features=1, bias=True)\n)\ntensor([[0.3848],\n        [0.3733]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))\n",
    "\n",
    "print(net)\n",
    "X = torch.rand(2, 4)\n",
    "print(net(X))\n",
    "Y = net(X).sum()"
   ]
  },
  {
   "source": [
    "#### (1) 对于``Sequential实例中``含模型参数的层,可以通过Module类的``parameters()``或者``named_parameters方法``来``访问``所有参数(``迭代器``的方式)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'generator'>\n0.weight torch.Size([3, 4])\n0.bias torch.Size([3])\n2.weight torch.Size([1, 3])\n2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(type(net.named_parameters()))\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\nbias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "# 通过迭代器返回的参数名字自动加上了层数的索引作为前缀.\n",
    "# 通过索引可以对net中单层的参数进行访问\n",
    "\n",
    "for name, param in net[0].named_parameters():\n",
    "    print(name, param.size(), type(param))\n"
   ]
  },
  {
   "source": [
    "#### (2) 如果一个``Tensor是Parameter``,那么它会``自动被添加到模型的参数列表里``.  \n",
    "\n",
    "可见param的type为``torch.nn.parameter.Parameter``,它是``Tensor的子类``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weight1\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        \n",
    "        # 一个Tensor是Parameter\n",
    "        self.weight1 = nn.Parameter(torch.rand(20, 20))\n",
    "        # 一个Tensor不是Parameter\n",
    "        self.weight2 = torch.rand(20, 20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "n = MyModel()\n",
    "for name, param in n.named_parameters():\n",
    "    print(name) # 只有weight1,而weight2未出现在参数列表中."
   ]
  },
  {
   "source": [
    "#### (3) 既然Parameter是一个Tensor,那么Tensor有的``属性``它都有.  \n",
    "可以根据``data``来范文参数数值,用``grad``访问参数梯度."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.1286, -0.3000, -0.2277,  0.4783],\n        [ 0.1060, -0.1807, -0.2375,  0.3358],\n        [ 0.0597,  0.2626, -0.4754, -0.3147]])\nNone\ntensor([[-0.1139, -0.0493, -0.1180, -0.1265],\n        [-0.4102, -0.1776, -0.4249, -0.4556],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "weight_0 = list(net[0].parameters())[0]\n",
    "print(weight_0.data)\n",
    "print(weight_0.grad) # 输出None,反向传播前梯度为None\n",
    "Y.backward()\n",
    "print(weight_0.grad)"
   ]
  },
  {
   "source": [
    "### 2.2 初始化模型参数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.weight tensor([[-0.0126,  0.0147,  0.0057, -0.0053],\n        [-0.0026, -0.0098, -0.0060, -0.0025],\n        [ 0.0175, -0.0090, -0.0030, -0.0013]])\n2.weight tensor([[0.0081, 0.0103, 0.0040]])\n"
     ]
    }
   ],
   "source": [
    "# 将权重参数初始化成均值为0,标准差为0.01的正态分布随机数,并将偏差参数清0.\n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "\n",
    "        init.normal_(param, mean=0, std=0.01)\n",
    "        \n",
    "        print(name, param.data)"
   ]
  },
  {
   "source": [
    "### 2.3 自定义初始化方法\n",
    "对于init模块中没有提供的初始化方法,可以自行实现一个初始化方法."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Pytorch是如何实现初始化方法的.  \n",
    "``torch.nn.init.normal_``\n",
    "* 这个过程``不记录梯度``.\n",
    "* 这就是一个``inplace改变Tensor值``的函数."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### ``in-place operation``:  \n",
    "在pytorch中这是指,在改变一个tensor的值的时候,不经过复制操作,直接``在原来的内存上改变它的值``.可以理解为``原地操作符``.  \n",
    "在pytorch中,通常``加后缀\"_\"来表示``原地in-place operation.  \n",
    "python里面的+=, *= 也是in-place operation.\n",
    "\n",
    "#### 补充知识:\n",
    "nn.ReLU(inplace=True) #inplace为True, 默认为False.  \n",
    "其含义是:是否将计算得到的值直接覆盖之间的值(原地操作).  \n",
    "例如:x = x + 1   \n",
    "a) 原地操作,对原值进行加1操作后得到的值,直接赋值给x.  \n",
    "b) 中间变量操作, 先找一个中间变量y,y=x+1, 然后y = x.    \n",
    "原地操作,能够节省运算内存,不用多存储其他变量."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_(tensor, mean=0, std=1):\n",
    "    with torch.no_grad():\n",
    "        return tensor.normal_(mean, std)"
   ]
  },
  {
   "source": [
    "* 类似的,实现一个自定义的初始化方法.  \n",
    "初始化方法:令权重有一半概率初始化为0,有另一半概率初始化为\\[-10,-5\\]和\\[5, 10\\]两个区间."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.weight tensor([[-0.0000,  0.0000,  0.0000, -9.4369],\n        [-0.0000,  0.0000, -7.8316,  0.0000],\n        [ 8.8992,  5.4346, -8.3286, -0.0000]])\n2.weight tensor([[-5.8922,  8.0111, -5.4810]])\n"
     ]
    }
   ],
   "source": [
    "def init_weight_(tensor):\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10, 10)\n",
    "        tensor *= (tensor.abs() >= 5).float()\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init_weight_(param)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "source": [
    "### 2.4 共享模型参数\n",
    "有些情况下,希望在多个层之间共享模型参数.  \n",
    "* Module类的forward函数多次调用一个层,是共享参数的.  \n",
    "* 传入Sequential的模块是同一个Module实例的话,参数是共享的."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=1, out_features=1, bias=False)\n  (1): Linear(in_features=1, out_features=1, bias=False)\n)\n0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(1, 1, bias=False)\n",
    "net = nn.Sequential(linear, linear)\n",
    "print(net)\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    init.constant_(param, val=3)\n",
    "    print(name, param.data)"
   ]
  },
  {
   "source": [
    "在结果中,只返回了第0层的参数?  \n",
    "在``内存中``,这两个线性层其实是``一个对象``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\n"
     ]
    }
   ],
   "source": [
    "print(id(net[0]) == id(net[1]))\n",
    "print(id(net[0].weight) == id(net[1].weight))"
   ]
  },
  {
   "source": [
    "## 3.模型参数的延后初始化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.自定义层"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}