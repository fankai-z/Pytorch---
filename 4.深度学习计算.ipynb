{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('pt1': conda)",
   "display_name": "Python 3.8.5 64-bit ('pt1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "606908fc0c26d26e63c369289d4b4eb7ec00ae86ec85f16e9e0fe916ae62f542"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1.模型构造"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.1 继承Module类来构造模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* 可以基于``Sequential类``构造模型\n",
    "* 也可以基于``Module类``构造模型\n",
    "\n",
    "``Module``类是``nn``模块里提供的一个模型构造类,是所有神经网络结构的``基类``.通过继承这个基类来定义我们想要的模型."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "这里定义的MLP类``重载``了Module类的``__init__``函数和``forward``函数.分别用于``初始化模型参数``和定义``前向计算``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        # 调用MLP父类Module的构造函数来进行必要的初始化.\n",
    "        # 使用**kwargs定义参数时，kwargs将会接收一个positional argument后所有关键词参数的字典。\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784, 256) # 隐藏层\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 10) # 输出层\n",
    "    \n",
    "    # 定义前向计算\n",
    "    # 目的是根据输入x计算模型输出.\n",
    "    def forward(self, x):\n",
    "        a = self.act(self.hidden(x))\n",
    "        return self.output(a)\n",
    "    "
   ]
  },
  {
   "source": [
    "MLP类中无需定义反向传播函数.系统通过自动求梯度而``自动生成``反向传播所需的``backward``函数."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "可通过``实例化MLP类``得到模型变量net.  \n",
    "net(x)会调用继承自Module类的__call__函数,这个函数将调用MLP类定义的forward函数完成前向计算.  \n",
    "\n",
    "也即,实例化之后,MLP类中的``前向计算会自动完成``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLP(\n  (hidden): Linear(in_features=784, out_features=256, bias=True)\n  (act): ReLU()\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0675, -0.1139, -0.0486, -0.2151, -0.0018, -0.0563, -0.0357,  0.1635,\n",
       "         -0.2710,  0.0487],\n",
       "        [-0.1617,  0.0408, -0.0064, -0.0279, -0.0036, -0.0329,  0.0648,  0.1680,\n",
       "         -0.1194,  0.2270]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X = torch.rand(2, 784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "source": [
    "Module类是一个可供自由组建的部件.它的子类既可以是一个层(如Linear类),又可以是一个模型(如上面的MLP类),或者是模型的一个部分."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.2 Module的子类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "继承自Module类的一些可以方便的构造模型的类还有:  \n",
    "* Sequential类  \n",
    "* ModuleList类  \n",
    "* ModuleDict类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1)Sequential类  \n",
    "以更简单的方式定义模型．它``接受``一个子模块的``有序字典``，或者，一系列子模块作为参数逐一添加``Ｍodule的实例``.在前向计算时就是将这些实例按添加顺序逐一计算."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2)ModuleList类  \n",
    "``接收``一个``子模块的列表``作为输入,然后也可以类似List那样进行append和extend操作.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\n",
    "# net.append(nn.Linear(256, 10)) # # 类似List的append操作\n",
    "# print(net[-1])  # 类似List的索引访问\n",
    "# print(net)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "source": [
    "#### Question:  \n",
    "a) ``Sequential``和``MoudleList``的区别?  \n",
    "\n",
    "ModuleList仅仅储存各种模块的列表,模块之间没有联系,没有顺序,而且没有实现forward.  \n",
    "Sequential内的模块需要按照顺序排列,且要保证邻层的输入输出大小相匹配,内部forward功能已经实现.  \n",
    "\n",
    "b) ``ModuleList``的出现有什么作用?  \n",
    "\n",
    "让网络定义前向传播时更加灵活.  \n",
    "另外,加入到ModuleList中的模块会自动添加到网络中,看下面代码中的例子对比."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "net1:\ntorch.Size([10, 10])\ntorch.Size([10])\nnet2:\nModule_ModuleList(\n  (linears): ModuleList(\n    (0): Linear(in_features=10, out_features=10, bias=True)\n  )\n)\n=========\nModule_List()\n"
     ]
    }
   ],
   "source": [
    "class Module_ModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module_ModuleList, self).__init__()\n",
    "        # 加入到ModuleList中的模块自动添加到网络中.\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10)])\n",
    "\n",
    "class Module_List(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Module_List, self).__init__()\n",
    "        # 这个线性层没有出现在网络结构中.\n",
    "        self.linears = [nn.Linear(10, 10)]\n",
    "\n",
    "net1 = Module_ModuleList()\n",
    "net2 = Module_List()\n",
    "\n",
    "print(\"net1:\")\n",
    "for p in net1.parameters():\n",
    "    print(p.size())\n",
    "\n",
    "print(\"net2:\")\n",
    "for p in net2.parameters():\n",
    "    print(p)\n",
    "\n",
    "print(net1)\n",
    "print(\"=========\")\n",
    "print(net2)\n"
   ]
  },
  {
   "source": [
    "3)ModuleDict类  \n",
    "``接受``一个子模块的``字典``作为输入,然后可以类似字典那样进行添加和访问操作.  \n",
    "与ModuleList一样,仅仅存放了一个字典,forward函数需要自己实现,同时,ModuleDict里的模块也会自动添加到整个网络中."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ModuleDict(\n  (Linear): Linear(in_features=784, out_features=256, bias=True)\n  (act): ReLU()\n  (output): Linear(in_features=256, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# 以字典作为输入\n",
    "net = nn.ModuleDict({\n",
    "    'Linear':nn.Linear(784, 256),\n",
    "    'act':nn.ReLU(),\n",
    "})\n",
    "# 添加\n",
    "net['output'] = nn.Linear(256, 10)\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "### 1.3 构造复杂的模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 一个复杂一点的``FancyMLP``网络.  \n",
    "* 通过``get_constant``函数创建训练中不被迭代的参数,即``常数参数``.\n",
    "* 前向计算中,使用``Tensor``函数和Python的``控制流``,并多次``调用相同的层``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FancyMLP(\n  (linear): Linear(in_features=20, out_features=20, bias=True)\n)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(3.9904, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "class FancyMLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FancyMLP, self).__init__(**kwargs)\n",
    "\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False) # 不可训练参数(常数参数)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # 使用创建的常数参数,以及nn.functional中的relu函数和mm函数\n",
    "        x = nn.functional.relu(torch.mm(x, self.rand_weight.data) + 1)\n",
    "\n",
    "        # 复用全连接层.等价于两个全连接层共享参数\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # print(\"++++\")\n",
    "        # print(x.shape) #(2, 20)\n",
    "        # print(x.norm())\n",
    "        # print(x.norm().item())\n",
    "        while x.norm().item() > 1:\n",
    "            x /= 2\n",
    "        if x.norm().item() < 0.8:\n",
    "            x *= 10\n",
    "        return x.sum()\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net = FancyMLP()\n",
    "print(net)\n",
    "net(X)\n",
    "\n",
    "\n",
    "# 从结果看出,网络结构中只定义了一个linear层,但是前向计算中可以对linear层进行多次调用.\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "#### 定义多个继承自nn.Module的模块还可以作为Sequential的输入."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* 继承自Module类的模块可以作为``模型的一部分``,每个部分定义了``各自的前向计算过程``.每个部分可以作为Sequential的输入组合到一起,构成整个网络.\n",
    "\n",
    "* 印证前文所述:继承自Module类的子类是一个可供自由组建的部件.它可以是一个层,可以是一个模型,也可以是模型的一部分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): NestMLP(\n    (net): Sequential(\n      (0): Linear(in_features=40, out_features=30, bias=True)\n      (1): ReLU()\n    )\n  )\n  (1): Linear(in_features=30, out_features=20, bias=True)\n  (2): FancyMLP(\n    (linear): Linear(in_features=20, out_features=20, bias=True)\n  )\n)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-0.4560, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NestMLP, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential(nn.Linear(40, 30), nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "net = nn.Sequential(NestMLP(), nn.Linear(30, 20), FancyMLP())\n",
    "\n",
    "X = torch.rand(2, 40)\n",
    "print(net)\n",
    "net(X)\n"
   ]
  },
  {
   "source": [
    "## 2.模型参数的访问,初始化与共享"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.模型参数的延后初始化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.自定义层"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}